{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4Yv6VWFM+a1VFW20XFe6D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waqarmm/AI-Labs/blob/master/code_by_arbab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjz34Js7bGk7"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Classification_with_Neural_Networks_using_Tensorflow\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1z0ddq6svHLGsHLhRFUcyGndlCCM3ds9A\n",
        "\n",
        "# Lab Overview\n",
        "In this lab, you will learn how to apply neural networks for classification tasks using the Iris flower dataset. The Iris dataset contains measurements of four features of different iris flowers: sepal length, sepal width, petal length, and petal width. Our goal is to train a neural network model to classify the iris flowers into three different species: Setosa, Versicolor, and Virginica.\n",
        "\n",
        "## Step 1: Import Libraries\n",
        "We'll start by importing the necessary libraries. In this lab, we'll be using TensorFlow, NumPy, and scikit-learn.\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\"\"\"## Step 2: Load and Preprocess the Dataset\n",
        "Next, we'll load the Iris dataset and preprocess it. We'll split the dataset into training and testing sets and perform one-hot encoding on the target labels.\n",
        "\"\"\"\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "print(iris)\n",
        "\n",
        "# Split the dataset into features and labels\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "# We can also convert this X and Y array to Pandas DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data=X, columns=[\"Sepal_length\", \"Sepal_width\", \"Petal_length\", \"Petal_width\"])\n",
        "df[\"Species\"] = y\n",
        "df\n",
        "\n",
        "df.shape\n",
        "\n",
        "df.isnull().sum()\n",
        "\n",
        "# Perform one-hot encoding on the target labels\n",
        "encoder = OneHotEncoder(categories=\"auto\")\n",
        "y = encoder.fit_transform(y).toarray()\n",
        "y\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\"\"\"## Step 3: Build the Neural Network Model\n",
        "Now, we'll define and build our neural network model using TensorFlow. We'll use a simple architecture with two hidden layers.\n",
        "\"\"\"\n",
        "\n",
        "# Define the model architecture\n",
        "import tensorflow as tf\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(10, input_shape=(4,), activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\"\"\"## Step 4: Compile and Train the Model\n",
        "Next, we'll compile the model by specifying the loss function, optimizer, and evaluation metric. Then, we'll train the model on the training data.\n",
        "\"\"\"\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=8, verbose=1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract loss and accuracy values from the training history\n",
        "loss = history.history['loss']\n",
        "accuracy = history.history['accuracy']\n",
        "\n",
        "# Plot the loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(loss) + 1), loss, color='blue')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot the accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(accuracy) + 1), accuracy, color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\"\"\"## Step 5: Evaluate the Model\n",
        "After training, we'll evaluate the model's performance on the testing data and calculate the accuracy.\n",
        "\"\"\"\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n",
        "\n",
        "\"\"\"## Step 6: Make Predictions\n",
        "Finally, we can use the trained model to make predictions on new data.\n",
        "\"\"\"\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions)\n",
        "\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "print(predicted_labels)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(np.argmax(y_test, axis=1), predicted_labels)\n",
        "\n",
        "# Plot the confusion matrix using a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ]
}